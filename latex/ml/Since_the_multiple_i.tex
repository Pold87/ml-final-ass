I trained models and made predictions on all possible combinations of imputed training and test sets. This resulted in 25 ``preliminary submissions" that had to be aggregated to obtain one final submission. For the aggregation, I used non-weighted majority voting on the predictions (0 or 1) of the single datasets.

TODO:
The power of ensemble learning can also be applied to the final submissions to Kaggle: they can be aggregated again by using majority voting, thus, I took three submissions to combine them. This resulted in a slightly improved performance.