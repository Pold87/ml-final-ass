\documentclass{article}

\usepackage[style=authoryear,backend=biber]{biblatex}
% Set library for biblatex
\bibliography{library}


\usepackage{booktabs}
\usepackage{float}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}

\title{Machine Learning -- Final Report}
\author{Volker Strobel}
\date{\today}


% THE REPORT

% In addition to participating in the competition, we expect you to
% provide us with a report on your work that at least contains the
% following :

% - A clear description of the complete and, possibly, best-performing
% method you implemented;

% - Clear arguments for the different choices you made in compiling
% your classifier;

% - Sufficient experimental results, learning curves, error bars, or
% whatever else you need to convince the reader that little
% improvement will be possible beyond the system that your method is
% currently performing. Your final score is of course also there to
% see whom of your colleagues you have beaten and, for us, to use in
% the final decision on your grade.

% - Any references that have been used.


\begin{document}
\maketitle

\begin{abstract}
  This reports presents the techniques and results of a classification
  problem involving missing data as well as a mixture of categorical
  and continuous data. The problem of missing data points is being
  addressed by Multiple Imputation Chained Equations. The
  classification is conducted using the AdaBoost classifier. The
  method has been locally evaluated using cross-validation and
  remotely on a hold-out test-set using the Kaggle platform.
\end{abstract}

\section{The Challenge}
\label{sec:introduction}

The given problem is to predict, whether a person earns over EUR 40k a
year (Table~\ref{tab:features}). The competition involves three main
challenges:
\begin{itemize}
\item Handling missing data points
\item Mixture of categorical and continuous independent variables
\item Classification of the output variable
\end{itemize}

Section~\ref{sec:background} briefly compares different methods for
missing data. In Section~\ref{sec:analysis}, we analyze and visualize
the structure of the data, to set the stage for the
feature-extraction-to-classification pipeline. In
Section~\ref{sec:methods}, the used method---Adaboost
classification---is described in detail. Section~\ref{sec:results}
describes the results obtained during cross-validation and on
Kaggle. In Section~\ref{sec:discussion}, the results are discussed.

\section{Background}
\label{sec:background}

Missing data values are a common problem in statistics. The failure of
sensors, or the concealment of data impede machine learning
accuracy. However, methods have been put forth for the imputation of
missing data points, such as case deletion or single mean
imputation~\ref{schafer2002missing}. However, such simple methods
often discard useful information and are not favorable if a large part
of the data is missing.


\section{Analysis}
\label{sec:analysis}

In order to motivate the used classifier, design and technique
choices, we start with an in-depth analysis and visualization of the
given dataset.

Table~\ref{tab:features} shows the discrete and continuous
features. 

\begin{table}[h]
  \centering
  \begin{tabular}{cc}
    \toprule
  Categorical  & Continuous                     \\
    \midrule
    work class & age                            \\
    education  & number of years of education   \\
marital status & income from investment sources \\
occupation     & losses from investment sources \\
relationship   & working hours per week         \\
race           &                                \\
sex            &                                \\
native country &                                \\
      \bottomrule
  \end{tabular}
  \caption{Overview of the used features}
  \label{tab:features}
\end{table}

\begin{figure}[H]
  \centering
  
  \caption{Scattermatrix of the given dataset.}
  \label{fig:scatter}
\end{figure}

\section{Methods}
\label{sec:methods}

\subsection{Missing Data Values}

A first analysis shows that in total $19.6\,\%$ of the data values are
missing, with $23.1\,\%$ for the variables workclass and occupation,
and $18.8\,\%$ for the remaining variables. 

For the possibly best performance, we would like to show that the data
is missing completely at random (MCAR). Therefore, we conduct Little's
MCAR test \cite{little1988test} to analyze the interaction structure
of the variables.

Since the relative frequency of missing data points is rather large,
simple methods, like mean imputation are likely to give a suboptimal
result. Therefore, data points were imputed busing a technique called
Multivariate Imputation by Chained Equations
(MICE)~\cite{buuren2011mice} using the \texttt{R} package
\texttt{mice} (Version~2.25). The algorithm uses multiple imputations,
which allows for incorporating the statistical uncertainty in the
imputed values. It is a flexible approch that can aldo handle
continuous and categorical variables. The general idea is to model the
conditional probability of each missing variable given the remaining
variables.

The algorithm works as follows~\cite{azur2011multiple}:
\begin{enumerate}
\item In the beginning, a first simple imputation is performed. To
  this end, all missing data points are replaced by random sampling
  with replacement from the observed datapoints.
\item One variable is selected at random, for example
  ``occupation''. An ``intermediate'' regression model is build, using
  the remaining variables as predictors and occupation as target
  value. The chosen model is dependent on the target value. I used a
  logistic regression for binary data, a polytomous regression model
  for categorical data, and predictive mean matching for numerical
  data. The missing values in occupation are replaced by the
  predictions of the model
\item The previous step is performed for each variable with missing
  data. The models that are build use the imputed and existing values
  for the training of the model. If all variables were predicted, one
  cycle is completed.
\item Several cycles are performed to stabilize the imputation
  results. I used $c = 5$ cycles.
\item The entire procedure is performed multiple times to yield
  multiple imputed datasets. Due to the random factors, the imputated
  values will be different, while the non-missing data entries will be
  the same. I used $m = 5$ imputations.
\end{enumerate}

The MICE algorithm was used on a combined dataset, by combining the
training and the testset to one large dataset. This should increase
the quality of the impuated values since more training examples can be
used for building the intermediate models (Step 2 in the algorithm
outline).

\subsection{Pooling}

Since the multiple imputation methods yields multiple datasets ($m =
5$), the predictions have to be aggregated. For this, majority voting
was used based on the predictions (0 or 1) of the single dataset.

\subsection{Dummy Variables}

Seven of the twelve measured variables are qualitative (workclass,
education, marital status, occupation, relationship, sex, native
country)---that is, they are measured only at the nominal level. Since
measurements on the nominal level do not allow for a particular
ordering, a proxy method has to be used. Therefore, we define dummy
variables which take the value 0 or 1 that indicate if a certain
category is present. Defining $N$ dummy variables for the $N$
different possible values of a categorical feature allows for
capturing the full information in the orginal unmodified dataset and
use qualitative data in a straight-forward manner in regression models
that are usually based on decision boundaries or linear relationships.

After transforming the dataset, 104 variables were obtained, which
largely increases the size of the dataset.

\subsection{Cross-Validation}
\label{sec:cross}

To evaluate submissions and determine the rank of participants, the
Kaggle challenge used the 1/0 loss. While the score in the challenge
is the best validation of the methods employed, only one submission
could be submitted per participant and day. In order to evaluate the
used methods more frequently, local 5-fold cross-validation was
implemented. The cross validation was performed per dataset of the
imputation result, that is, no cross-dataset cross-validation was
performed. Therefore, in the case of five imputations, 25
cross-validation results were obtained. 

The cross-validation achieved similar results to the public
Kaggle evaluation (Table~\ref{tab:localpublic}). 

\begin{table}[H]
  \centering
  \begin{tabular}{lll}
    \toprule
    Method & Local Score & Global Score\\
    \midrule
    AdaBoost with 1 imputation & 0.83886 & 0.84205\\
    Random Forest with five imputations & 0.82726 & 0.85056\\
    SVM with five imputations & 0.84413 & 0.85494\\
    \bottomrule
  \end{tabular}
  \caption{Comparing local and public scores}
  \label{tab:localpublic}
\end{table}

\subsection{The Classifier}

As can be seen in Section~\ref{sec:cross}, different classifiers were
tested for the given problem. The best performing one was a support
vector machine (SVM). For running the algorithm \texttt{Python~2.7.11}
was used with the package \texttt{scikit-learn} in Version~0.17.0.

Support vector machines build a $D-1$-dimensional hyperplane that
tries to maximize the margin between the two classes. The support
vectors are the samples on the margin for the positive class and the
negative class. The maximum-margin hyperplane is the hyperplane
``right in the middle'' between the support vectors.

The classification function is $h(x) = g(w^Tx + b)$, with $w$ being
the coefficient vector and $b$ the intercept term. The function $g(z)$
is defined as $g(z) = 1$, if $z \geq 0$ and $g(z) = -1$ otherwise. The
goal is to find $w, b$ such that the resulting decision boundary
maximizes the margin.

\subsubsection{The kernel}
\label{sec:kernel}

To extend SVMs to non-linear classification, the \emph{kernel trick}
can be used.

\section{Feature-extraction-to-classification Pipeline}

\begin{figure}[h]
  \centering
% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, 
    text width=5em, text centered, rounded corners, minimum
    height=4em, node distance=3cm]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]
  \begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (train) {NaN Training set};
    \node [block, below of=train] (test) {NaN Test set};
    \node [block, right of=train] (stacked) {NaN Combined set};
    \node [block, right of=stacked] (full) {Full Combined set 1};
    \node [block, right below of=stacked] (full) {Full Combined set 2};
    % Draw edges
    \path [line] (train) -- (stacked);
    \path [line] (test) -- (stacked);
    \path [line] (stacked) -- (full) node [midway] {mice};
\end{tikzpicture}
  \caption{The pipeline}
  \label{fig:pipeline}
\end{figure}

\section{Results}
\label{sec:results}

In total, I made XX submissions during the competition. The best
submission resulted in an accuracy of XX, which corresponds to
position XX out of XX participants. 

Using a support vector machine, random forest regression, and AdaBoost
classifier resulted in very similar results. 

\section{Discussion}
\label{sec:discussion}

In summary, the main challenge lay in handling the missing data
values. Afterward, any standard regression technique could be
used. The use of different regression techniques had no substantial
influence on the achieved accuracy.

My code for this competition can be found at:\\
\url{https://github.com/Pold87/ml-final-ass}


\printbibliography
\appendix

\end{document}
